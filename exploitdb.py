import re
import json
import logging
import requests
import pathlib
from datetime import datetime, timedelta
from bs4 import BeautifulSoup as bs
from retry.api import retry
from requests.exceptions import ConnectTimeout, ReadTimeout
from random import randint, shuffle
from time import sleep


logger = logging.getLogger(__name__)
logging.basicConfig(
    format='%(asctime)s - %(name)s - [%(levelname)s] %(message)s',
    level=logging.INFO
)

class Config:
    http_proxy = None
    https_proxy = None

config = Config()
proxies = None
if config.http_proxy or config.https_proxy:
    proxies = {
        'http': f'http://{config.http_proxy}',
        'https': f'https://{config.https_proxy}'
    }
user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
base_url = 'https://www.exploit-db.com/'
cve_pattern = r"(CVE\-\d{4}\-\d*)"

@retry((ConnectTimeout, ReadTimeout), tries=10, delay=30, backoff=5)
def query_raw(ref_id :int):
    api_url = f'{base_url}raw/{ref_id}'
    logger.info(api_url)
    resp = requests.get(
        api_url,
        proxies=proxies,
        headers={
            'user-agent': user_agent,
            'referer': f'{base_url}exploits/{ref_id}'
        },
        timeout=3
    )
    if resp.status_code != 200:
        logger.error(f'status_code {resp.status_code}')
        return None

    return resp.text

def html_to_dict(html_content :str):
    meta_title = None
    keywords = None
    cve_id = None
    platform = None
    edb_type = None
    meta_author = None
    meta_author_url = None
    meta_published_time = None
    soup = bs(html_content, 'html.parser')
    
    meta_tag = soup.find(property='og:title')
    if meta_tag:
        meta_title = meta_tag.get("content")
    meta_tag = [item['content'] for item in soup.select('meta[name=keywords]')]
    if meta_tag:
        meta_keywords = meta_tag[0]
        if ',' in meta_keywords:
            keywords_arr = meta_keywords.split(',')
            if len(keywords_arr) == 2:
                platform, edb_type = keywords_arr
            if len(keywords_arr) == 3:
                platform, edb_type, keywords = keywords_arr
            if len(keywords_arr) > 3:
                platform, edb_type, *keywords = keywords_arr
                keywords = ','.join(keywords)
        else:
            keywords = meta_keywords

        cve_id = None
        matches = re.search(cve_pattern, keywords)
        if matches is not None:
            cve_id = matches.group(1)
    meta_tag = soup.find(property='article:author')
    if meta_tag:
        meta_author = meta_tag.get("content")
    meta_tag = soup.find(property='article:authorUrl')
    if meta_tag:
        meta_author_url = meta_tag.get("content")
    meta_tag = soup.find(property='article:published_time')
    if meta_tag:
        meta_published_time = meta_tag.get("content")

    return {
        'cve': cve_id,
        'published': meta_published_time,
        'title': meta_title,
        'author': meta_author,
        'author_url': meta_author_url,
        'platform': platform,
        'type': edb_type,
        'keywords': keywords if cve_id is None else keywords.replace(cve_id, '').strip(),
    }

@retry((ConnectTimeout, ReadTimeout), tries=10, delay=30, backoff=5)
def query_bulk(start :int, length :int = 15) -> dict:
    timestamp = int((datetime.utcnow() - datetime(1970, 1, 1)) / timedelta(seconds=1)*1000)
    edb_url = f'{base_url}?draw=2&columns%5B0%5D%5Bdata%5D=date_published&columns%5B0%5D%5Bname%5D=date_published&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=download&columns%5B1%5D%5Bname%5D=download&columns%5B1%5D%5Bsearchable%5D=false&columns%5B1%5D%5Borderable%5D=false&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=application_md5&columns%5B2%5D%5Bname%5D=application_md5&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=false&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=verified&columns%5B3%5D%5Bname%5D=verified&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=false&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=description&columns%5B4%5D%5Bname%5D=description&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=false&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=type_id&columns%5B5%5D%5Bname%5D=type_id&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=false&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=platform_id&columns%5B6%5D%5Bname%5D=platform_id&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=false&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=author_id&columns%5B7%5D%5Bname%5D=author_id&columns%5B7%5D%5Bsearchable%5D=false&columns%5B7%5D%5Borderable%5D=false&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=code&columns%5B8%5D%5Bname%5D=code.code&columns%5B8%5D%5Bsearchable%5D=true&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B9%5D%5Bdata%5D=id&columns%5B9%5D%5Bname%5D=id&columns%5B9%5D%5Bsearchable%5D=false&columns%5B9%5D%5Borderable%5D=true&columns%5B9%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B9%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=9&order%5B0%5D%5Bdir%5D=desc&start={start}&length={length}&search%5Bvalue%5D=&search%5Bregex%5D=false&author=&port=&type=&tag=&platform=&_={timestamp}'
    session = requests.Session()
    try:
        resp = session.get(
            edb_url,
            proxies=proxies,
            headers={
                'x-requested-with': 'XMLHttpRequest',
                'user-agent': user_agent,
                'referer': base_url
            },
            allow_redirects=False,
            timeout=10
        )
        code = resp.status_code
    except ReadTimeout:
        code = 504

    if code != 200:
        logger.warning(f'{code} {edb_url}')
        return None

    return json.loads(resp.text)
    
@retry((ConnectTimeout, ReadTimeout), tries=10, delay=30, backoff=5)
def query_site(ref_id :int, original_data :dict = {}) -> dict:
    edb_url = f'{base_url}exploits/{ref_id}'
    session = requests.Session()
    try:
        resp = session.head(
            edb_url,
            proxies=proxies,
            headers={
                'user-agent': user_agent,
                'referer': base_url
            },
            allow_redirects=False,
            timeout=3
        )
        code = resp.status_code
    except ReadTimeout:
        code = 504

    ret = {}
    if code == 200:
        resp = session.get(
            edb_url,
            proxies=proxies,
            headers={
                'user-agent': user_agent,
                'referer': base_url
            },
            allow_redirects=False,
            timeout=10
        )
        code = resp.status_code
        ret = html_to_dict(resp.text)

    else:
        logger.warning(f'{code} {edb_url}')
    
    ret['edb_id'] = ref_id
    ret['url'] = edb_url
    ret['http_code'] = code
    ret['created_at'] = original_data['created_at'] if 'created_at' in original_data else datetime.utcnow().isoformat()
    ret['last_checked'] = datetime.utcnow().isoformat()
    ret['download_url'] = f'{base_url}download/{ref_id}'
    if 'exploit_exists' not in original_data:
        raw_file = pathlib.Path(f'exploit-db/raw/{ref_id}')
        if not raw_file.is_file():
            sleep(randint(3,6))
            raw = query_raw(ref_id)
            if raw is not None and raw:
                raw_file.write_text(raw)
                ret['exploit_exists'] = True

    return ret

def temp_site(file_handle, data):
    if 'exploit' in data:
        data['download_url'] = data['exploit']
        del data['exploit']

    raw_file = pathlib.Path(f'exploit-db/raw/{data["edb_id"]}')
    data['exploit_exists'] = raw_file.is_file()
    if 'http_code' not in data:
        session = requests.Session()
        try:
            resp = session.head(
                data['url'],
                proxies=proxies,
                headers={
                    'user-agent': user_agent,
                    'referer': base_url
                },
                allow_redirects=False,
                timeout=3
            )
            code = resp.status_code
        except ReadTimeout:
            code = 504

    data['http_code'] = code
    if data['http_code'] == 200 and not data['exploit_exists']:
        sleep(randint(3,6))
        raw = query_raw(data["edb_id"])
        if raw is not None and raw:
            raw_file.write_text(raw)
            data['exploit_exists'] = True
    file_handle.write_text(json.dumps(data, default=str, sort_keys=True))

def main():
    next_id = 1
    last_id = 50200
    ids = list(range(next_id, last_id))
    shuffle(ids)
    for check_id in ids:
        not_today = datetime.utcnow() - timedelta(seconds=86400)
        submission_file = pathlib.Path(f'exploit-db/submissions/{check_id}.json')
        if submission_file.is_file():
            last_modified = datetime.fromtimestamp(submission_file.stat().st_mtime)
            json_data = json.loads(submission_file.read_text())
            if last_modified > not_today:
                temp_site(submission_file, json_data) #TODO remove
                continue
            if 'published' in json_data: # We already have the data
                continue
            if json_data['http_code'] != 404:
                submission_file.touch(exist_ok=True)

        sleep(randint(3,6))
        data = query_site(check_id, json_data)
        if data is not None:
            submission_file.write_text(json.dumps(data, default=str, sort_keys=True))

def temp_bulk(file_handle, data):
    if 'exploit' in data:
        data['download_url'] = data['exploit']
        del data['exploit']

    if 'cve' in data and data['cve'] is not None:
        cve = data['cve']
        if not isinstance(data['cve'], list):
            data['cve'] = [cve]

    raw_file = pathlib.Path(f'exploit-db/raw/{data["edb_id"]}')
    data['exploit_exists'] = raw_file.is_file()
    if not data['exploit_exists']:
        sleep(randint(3,6))
        raw = query_raw(data["edb_id"])
        if raw is not None and raw:
            raw_file.write_text(raw)
            data['exploit_exists'] = True
    file_handle.write_text(json.dumps(data, default=str, sort_keys=True))

def save_bulk(data :dict) -> dict:
    del data['download']
    cve = []
    if data['code']:
        for code in data['code']:
            # find exploit-db/submissions -name '*.json' -exec jq -r '._raw.code[].code_type' {} \; 2>/dev/null
            if code['code_type'] == 'cve':
                cve.append(f"CVE-{code['code']}")

    original_data = {}
    submission_file = pathlib.Path(f"exploit-db/submissions/{data['id']}.json")
    if submission_file.is_file():
        original_data = json.loads(submission_file.read_text())
        temp_bulk(submission_file, original_data) #TODO remove

    ret = {
        "_raw": data,
        "edb_id": int(data['id']),
        "verified": data['verified'] == 1,
        "url": f"{base_url}exploits/{data['id']}",
        "cve": cve,
        "published": data['date_published'],
        "title": data['description'][1],
        "author": data['author']['name'],
        "author_url": f"{base_url}?author={data['author']['id']}",
        "platform": data['platform']['platform'],
        "type": data['type']['display'],
        "download_url": f"{base_url}download/{data['id']}",
        "created_at": original_data['created_at'] if 'created_at' in original_data else datetime.utcnow().isoformat(),
        "last_checked": datetime.utcnow().isoformat(),
    }
    
    raw_file = pathlib.Path(f'exploit-db/raw/{data["id"]}')
    data['exploit_exists'] = raw_file.is_file()
    if not data['exploit_exists']:
        sleep(randint(3,6))
        raw = query_raw(data['id'])
        if raw is not None and raw:
            raw_file.write_text(raw)
            ret['exploit_exists'] = True

    submission_file.write_text(json.dumps(ret, default=str, sort_keys=True))
    return ret

def bulk():
    start = 0
    length = 15
    response = query_bulk(start, length)
    if response is None:
        return
    data = response.get('data', [])
    if not data:
        return
    for edb in data:
        save_bulk(edb)
    break_on = int(data[0]['id']) + length
    while start <= break_on:
        start += length
        sleep(randint(4,8))
        response = query_bulk(start, length)
        if response is None:
            continue
        data = response.get('data', [])
        if not data:
            continue
        for edb in data:
            save_bulk(edb)

if __name__ == "__main__":
    bulk()