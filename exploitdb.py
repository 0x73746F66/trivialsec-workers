import re
import json
import logging
import pathlib
from datetime import datetime, timedelta
from random import randint, shuffle
from time import sleep
import requests
from requests.exceptions import ConnectTimeout, ReadTimeout
from bs4 import BeautifulSoup as bs
from retry.api import retry


session = requests.Session()
logger = logging.getLogger(__name__)
logging.basicConfig(
    format='%(asctime)s - %(name)s - [%(levelname)s] %(message)s',
    level=logging.INFO
)

class Config:
    http_proxy = None
    https_proxy = None

config = Config()
PROXIES = None
if config.http_proxy or config.https_proxy:
    PROXIES = {
        'http': f'http://{config.http_proxy}',
        'https': f'https://{config.https_proxy}'
    }
USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
BASE_URL = 'https://www.exploit-db.com/'
DATAFILE_DIR = 'datafiles/exploit-db/submissions'
RAWFILE_DIR = 'datafiles/exploit-db/raw'
CVE_PATTERN = r"(CVE\-\d{4}\-\d*)"

@retry((ConnectTimeout, ReadTimeout), tries=10, delay=30, backoff=5)
def query_raw(ref_id :int):
    api_url = f'{BASE_URL}raw/{ref_id}'
    logger.info(api_url)
    resp = requests.get(
        api_url,
        proxies=PROXIES,
        headers={
            'user-agent': USER_AGENT,
            'referer': f'{BASE_URL}exploits/{ref_id}'
        },
        timeout=3
    )
    if resp.status_code != 200:
        logger.error(f'status_code {resp.status_code}')
        return None

    return resp.text

def html_to_dict(html_content :str):
    meta_title = None
    keywords = None
    cve_id = None
    platform = None
    edb_type = None
    meta_author = None
    meta_author_url = None
    meta_published_time = None
    soup = bs(html_content, 'html.parser')
    
    meta_tag = soup.find(property='og:title')
    if meta_tag:
        meta_title = meta_tag.get("content")
    meta_tag = [item['content'] for item in soup.select('meta[name=keywords]')]
    if meta_tag:
        meta_keywords = meta_tag[0]
        if ',' in meta_keywords:
            keywords_arr = meta_keywords.split(',')
            if len(keywords_arr) == 2:
                platform, edb_type = keywords_arr
            if len(keywords_arr) == 3:
                platform, edb_type, keywords = keywords_arr
            if len(keywords_arr) > 3:
                platform, edb_type, *keywords = keywords_arr
                keywords = ','.join(keywords)
        else:
            keywords = meta_keywords

        cve_id = None
        matches = re.search(CVE_PATTERN, keywords)
        if matches is not None:
            cve_id = matches.group(1)
    meta_tag = soup.find(property='article:author')
    if meta_tag:
        meta_author = meta_tag.get("content")
    meta_tag = soup.find(property='article:authorUrl')
    if meta_tag:
        meta_author_url = meta_tag.get("content")
    meta_tag = soup.find(property='article:published_time')
    if meta_tag:
        meta_published_time = meta_tag.get("content")

    return {
        'cve': cve_id,
        'published': meta_published_time,
        'title': meta_title,
        'author': meta_author,
        'author_url': meta_author_url,
        'platform': platform,
        'type': edb_type,
        'keywords': keywords if cve_id is None else keywords.replace(cve_id, '').strip(),
    }

@retry((ConnectTimeout, ReadTimeout), tries=10, delay=30, backoff=5)
def query_bulk(start :int, length :int = 15):
    timestamp = int((datetime.utcnow() - datetime(1970, 1, 1)) / timedelta(seconds=1)*1000)
    edb_url = f'{BASE_URL}?draw=2&columns%5B0%5D%5Bdata%5D=date_published&columns%5B0%5D%5Bname%5D=date_published&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=download&columns%5B1%5D%5Bname%5D=download&columns%5B1%5D%5Bsearchable%5D=false&columns%5B1%5D%5Borderable%5D=false&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=application_md5&columns%5B2%5D%5Bname%5D=application_md5&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=false&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=verified&columns%5B3%5D%5Bname%5D=verified&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=false&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=description&columns%5B4%5D%5Bname%5D=description&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=false&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=type_id&columns%5B5%5D%5Bname%5D=type_id&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=false&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=platform_id&columns%5B6%5D%5Bname%5D=platform_id&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=false&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=author_id&columns%5B7%5D%5Bname%5D=author_id&columns%5B7%5D%5Bsearchable%5D=false&columns%5B7%5D%5Borderable%5D=false&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=code&columns%5B8%5D%5Bname%5D=code.code&columns%5B8%5D%5Bsearchable%5D=true&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B9%5D%5Bdata%5D=id&columns%5B9%5D%5Bname%5D=id&columns%5B9%5D%5Bsearchable%5D=false&columns%5B9%5D%5Borderable%5D=true&columns%5B9%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B9%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=9&order%5B0%5D%5Bdir%5D=desc&start={start}&length={length}&search%5Bvalue%5D=&search%5Bregex%5D=false&author=&port=&type=&tag=&platform=&_={timestamp}'
    logger.info(f'start={start} length={length} timestamp={timestamp}')
    try:
        resp = session.get(
            edb_url,
            proxies=PROXIES,
            headers={
                'x-requested-with': 'XMLHttpRequest',
                'user-agent': USER_AGENT,
                'referer': BASE_URL
            },
            allow_redirects=False,
            timeout=10
        )
        code = resp.status_code
    except ReadTimeout:
        code = 504

    if code != 200:
        logger.warning(f'{code} {edb_url}')
        return None

    return json.loads(resp.text)

@retry((ConnectTimeout, ReadTimeout), tries=10, delay=30, backoff=5)
def query_single(ref_id :int, original_data :dict):
    if not isinstance(original_data, dict):
        original_data = {}
    edb_url = f'{BASE_URL}exploits/{ref_id}'
    logger.info(f'query_single data ref_id={ref_id}')
    try:
        resp = session.head(
            edb_url,
            proxies=PROXIES,
            headers={
                'user-agent': USER_AGENT,
                'referer': BASE_URL
            },
            allow_redirects=False,
            timeout=3
        )
        code = resp.status_code
    except ReadTimeout:
        code = 504

    ret = {}
    if code == 200:
        logger.info(f'query_single raw file ref_id={ref_id}')
        resp = session.get(
            edb_url,
            proxies=PROXIES,
            headers={
                'user-agent': USER_AGENT,
                'referer': BASE_URL
            },
            allow_redirects=False,
            timeout=10
        )
        code = resp.status_code
        ret = html_to_dict(resp.text)

    else:
        logger.warning(f'{code} {edb_url}')

    ret['edb_id'] = ref_id
    ret['url'] = edb_url
    ret['http_code'] = code
    ret['created_at'] = original_data.get('created_at') if 'created_at' in original_data else datetime.utcnow().isoformat()
    ret['last_checked'] = datetime.utcnow().isoformat()
    ret['download_url'] = f'{BASE_URL}download/{ref_id}'
    if 'exploit_exists' not in original_data:
        raw_file = pathlib.Path(f'{RAWFILE_DIR}/{ref_id}')
        if not raw_file.is_file():
            sleep(randint(3,6))
            raw = query_raw(ref_id)
            if raw is not None and raw:
                raw_file.write_text(raw)
                ret['exploit_exists'] = True

    return ret

def check_exploit_exists(file_handle, data):
    if 'exploit' in data:
        data['download_url'] = data['exploit']
        del data['exploit']

    raw_file = pathlib.Path(f'{RAWFILE_DIR}/{data["edb_id"]}')
    data['exploit_exists'] = raw_file.is_file()
    if 'http_code' not in data:
        logger.info(f'check_exploit_exists edb_id={data["edb_id"]}')
        try:
            resp = session.head(
                data['url'],
                proxies=PROXIES,
                headers={
                    'user-agent': USER_AGENT,
                    'referer': BASE_URL
                },
                allow_redirects=False,
                timeout=3
            )
            code = resp.status_code
        except ReadTimeout:
            code = 504

    data['http_code'] = code
    if data['http_code'] == 200 and not data['exploit_exists']:
        sleep(randint(3,6))
        raw = query_raw(data["edb_id"])
        if raw is not None and raw:
            raw_file.write_text(raw)
            data['exploit_exists'] = True
    file_handle.write_text(json.dumps(data, default=str, sort_keys=True))

def main():
    next_id = 1
    last_id = 50200
    ids = list(range(next_id, last_id))
    shuffle(ids)
    for check_id in ids:
        not_today = datetime.utcnow() - timedelta(seconds=86400)
        submission_file = pathlib.Path(f'{DATAFILE_DIR}/{check_id}.json')
        if submission_file.is_file():
            last_modified = datetime.fromtimestamp(submission_file.stat().st_mtime)
            json_data = json.loads(submission_file.read_text())
            if last_modified > not_today:
                check_exploit_exists(submission_file, json_data)
                continue
            if 'published' in json_data: # We already have the data
                continue
            if json_data['http_code'] != 404:
                submission_file.touch(exist_ok=True)

        sleep(randint(3,6))
        data = query_single(check_id, json_data)
        if data is not None:
            submission_file.write_text(json.dumps(data, default=str, sort_keys=True))

def check_exploit_exists_v2(file_handle, data):
    if 'exploit' in data:
        data['download_url'] = data['exploit']
        del data['exploit']

    if 'cve' in data and data['cve'] is not None:
        cve = data['cve']
        if not isinstance(data['cve'], list):
            data['cve'] = [cve]

    raw_file = pathlib.Path(f'{RAWFILE_DIR}/{data["edb_id"]}')
    data['exploit_exists'] = raw_file.is_file()
    if not data['exploit_exists']:
        sleep(randint(3,6))
        raw = query_raw(data["edb_id"])
        if raw is not None and raw:
            raw_file.write_text(raw)
            data['exploit_exists'] = True
    file_handle.write_text(json.dumps(data, default=str, sort_keys=True))

def save_bulk(data :dict) -> dict:
    del data['download']
    cve = []
    if data['code']:
        for code in data['code']:
            # find $DATAFILE_DIR -name '*.json' -exec jq -r '._raw.code[].code_type' {} \; 2>/dev/null
            if code['code_type'] == 'cve':
                cve.append(f"CVE-{code['code']}")

    original_data = {}
    submission_file = pathlib.Path(f"{DATAFILE_DIR}/{data['id']}.json")
    if submission_file.is_file():
        original_data = json.loads(submission_file.read_text())
        check_exploit_exists_v2(submission_file, original_data)

    ret = {
        "_raw": data,
        "edb_id": int(data['id']),
        "verified": data['verified'] == 1,
        "url": f"{BASE_URL}exploits/{data['id']}",
        "cve": cve,
        "published": data['date_published'],
        "title": data['description'][1],
        "author": data['author']['name'],
        "author_url": f"{BASE_URL}?author={data['author']['id']}",
        "platform": data['platform']['platform'],
        "type": data['type']['display'],
        "download_url": f"{BASE_URL}download/{data['id']}",
        "created_at": original_data['created_at'] if 'created_at' in original_data else datetime.utcnow().isoformat(),
        "last_checked": datetime.utcnow().isoformat(),
    }
    
    raw_file = pathlib.Path(f'{RAWFILE_DIR}/{data["id"]}')
    data['exploit_exists'] = raw_file.is_file()
    if not data['exploit_exists']:
        sleep(randint(3,6))
        raw = query_raw(data['id'])
        if raw is not None and raw:
            raw_file.write_text(raw)
            ret['exploit_exists'] = True

    submission_file.write_text(json.dumps(ret, default=str, sort_keys=True))
    return ret

def bulk():
    length = 1000
    response = query_bulk(0, length)
    if response is None:
        return
    data = response.get('data', [])
    if not data:
        return
    for edb in data:
        save_bulk(edb)
    break_on = int(data[0]['id']) + length
    start = length
    while start <= break_on:
        sleep(randint(4,8))
        response = query_bulk(start, length)
        if response is None:
            continue
        data = response.get('data', [])
        if not data:
            continue
        for edb in data:
            save_bulk(edb)
        start += length

def read_file(file_path :str):
    bulk_file = pathlib.Path(file_path)
    if bulk_file.is_file():
        for item in json.loads(bulk_file.read_text()).get('data', []):
            datafile = f"{DATAFILE_DIR}/{item['id']}.json"
            logger.info(datafile)
            edb_file = pathlib.Path(datafile)
            if edb_file.is_file():
                original_data = json.loads(edb_file.read_text())
                original_data |= item
                save_bulk(original_data)
                continue

            save_bulk(item)

if __name__ == "__main__":
    # read_file("exploitdb-response.json")
    bulk()
